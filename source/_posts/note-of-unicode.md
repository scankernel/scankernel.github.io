---
title: 浅谈unicode
auto_open: true
date: 2020-07-13 21:47:15
tags:
    - unicode
updated:
categories:
    - 技术
keywords:
description:
top_img:
comments:
cover:
toc: true
toc_number: true
copyright: true
---

> 虽然这几年来反复学习过unicode的相关知识，但是每次遇到和unicode相关的内容还是会有诸多疑惑，这几天正好有空，就上网查了很多资料，过程可以说是十分有趣，就记录下来了。

### 疑问的开始

[《core java》](https://www.amazon.com/Core-Java-II-Advanced-Features-11th/dp/0135166314)这本书里对char类型的描述如下：

从这段话里面我自己得到的结论是：unicode占用两个字节。

但我还是相当地疑惑，这时候我想起了几年前看过的一篇博文：[字符串和编码](https://www.liaoxuefeng.com/wiki/1016959663602400/1017075323632896)，二话不说，我又去阅读了一次。我从文章中截取一些重要信息：

- Unicode把所有语言都统一到一套编码里；
- Unicode标准也在不断发展，但最常用的是用两个字节表示一个字符（如果要用到非常偏僻的字符，就需要4个字节）。
- ASCII编码是1个字节，而Unicode编码通常是2个字节；
- 如果你写的文本基本上全部是英文的话，用Unicode编码比ASCII编码需要多一倍的存储空间，在存储和传输上就十分不划算。所以，本着节约的精神，又出现了把Unicode编码转化为“可变长编码”的UTF-8编码。
- 在计算机内存中，统一使用Unicode编码，当需要保存到硬盘或者需要传输的时候，就转换为UTF-8编码。

几年前阅读这篇博文的时候，我豁然开朗。如今再次阅读，我陷入了巨大的理解困境。有很多概念我无法理清：

1. Unicode把所有的语言都统一编写到一套编码里？真的是把所有语言都编进去了？一套编码是指什么？
2. unicode到底是用两个字节还是四个字节？
3. unicode， unicode编码，ascii，ascii字符集，ascii编码，utf-8编码的准确概念是什么？
4. 计算机内存中统一使用unicode编码？

带着这些疑问，我初步去查询了一下，在知乎问题["计算机中为何不直接使用 UTF-8 编码进行存储而要使用 Unicode 再转换成 UTF-8 ？"](https://www.zhihu.com/question/52346583)的最高赞[回答](https://www.zhihu.com/question/52346583/answer/130139771)中,我截取了一些信息：

- Windows记事本的“Unicode”这个名号其实相当有误导性。这个编码实际上是UTF-16 LE。
- Unicode的字符码，很少在计算机中直接用在存储和表达文本上。原因无他：太浪费空间了。
- Unicode字符码是32位，4个字节。平常使用的字符里，99%以上的字符都不会突破2个字节。
- 为了节省空间，人们就对Unicode的字符码再做二次编码，这就诞生了UTF-8，UTF-16，UTF-32等编码标准。

---
经过碎片式的查询，我的脑海里大约形成了这样的概念：

目前，Unicode字符码本身需要用32位（4个字节）来表示。直接使用unicode字符码有两个特点：`速度快`（省略了编码过程，所以速度快）和`占用内存空间大`（大多数时候我们计算机里面需要处理的字符只需要用1到2个字节就可以表示，如果直接用unicode就需要用4个字节来表示我们的字符）。这两个特点也就决定了对unicode的用法：

1. 如果追求的是速度，那么就直接使用unicode，比如在计算机内存中，就统一使用Unicode编码；
2. 如果追求的是“节省空间”，那么就对Unicode的字符码再做二次编码。这也就是为什么说“当需要保存到硬盘或者需要传输的时候，就转换为UTF-8编码”。

> 到了现在，似乎一切看起来都挺好，可是我内心深处，总觉得内存里直接使用unicode不太切合目前的世界。越想就越觉得特别是使用英文的国家里，需要在内存中用4倍的空间来表示字符，不太可能。

---
**我还是希望能够在一些官方的/正规的地方看到“在计算机内存中，统一使用Unicode编码”或者类似的表述。**

于是我在[stackoverflow](https://stackoverflow.com/questions/42168303/unicode-vs-ascii-memory)上找到了和我有类似疑问的老哥，这位老哥原问题是：

> What is the difference between Unicode and ASCII in terms of memory? How much memory does Unicode and ASCII take in memory? I know that in Unicode it depends on the encoding type (UTF-8, UTF-16 etc..). But I need deeper understanding.

粗劣翻译一下：
在计算机内存里，Unicode和ASCII有什么区别？ Unicode和ASCII各自占用多少内存？ 我知道在Unicode中，它取决于编码类型（UTF-8，UTF-16等）。 但是我需要更深刻的理解。

到了这里，我的终极问题呼之欲出了：既然计算机内存使用的是unicode，同时我又觉得直接**在内存里统一使用需要4个字节才能表示的unicode字符码不太现实**，那么在计算机内存里，一个unicode字符码到底占用多少个字节？

ps：我写这篇文章的时候其实已经觉得当时我的这些疑问好弱智，特别是在[stackoverflow](https://stackoverflow.com/questions/42168303/unicode-vs-ascii-memory)上其实人家已经给出答案，但那时我就是看不懂。这些“弱智”的疑问困惑了我好几天。（我太难了.gif


### 解惑之前

其实我之所以有诸多不明白的地方，就是因为字符编码相关的标准一直在变，比如以前的unicode码点（code point）只需要用2个字节表示，现在已经变成需要4个字节了。而且unicode指向太多含义了，在没有上下文的情况下，讨论到unicode，你可以认为是在说字符集，也可以认为是说unicode编码，甚至在windows平台下unicode可以是指UTF-16 LE编码。

字符编码本身在发展，unicode标准也一直在变，对unicode标准相关的专有名词理解不够深入，都会对理解字符编码造成极大的阻碍。

因此我希望我能够顺着计算机字符编码的历史长河，尽力指出什么地方变了，什么地方是不变的。讨论主线是从ascii到unicode。

但讨论之前，我门需要问自己一个问题：为什么需有有字符集这个东西？或者说，为什么需要编码？

### ascii

一个十分重要的基础知识：计算机[的CPU和内存]只能处理数字。

随着输出设备（屏幕，打印机等等）的产生，就促进了这么一个需求：人们希望计算机能够输出人类能够轻松阅读的文字。

但计算机只能处理数字（而且是二进制数字）是铁打的事实。这时候一种**从数字到字符的映射关系**就派上用场了。约定：数字65表示的是字符`A`，数字63代表字符`?`等等。以英语为母语的计算机科学家们发现只用128个数字就能表示所有常用的字符。因此，就形成了大名顶顶的ASCII字符集（ps：把ASCII字符集打印出来就形成ASCII码表），下面是ASCII码表：

这张表里面我们可以看出0～31表示的是控制字符，控制字符是不可以打印或显示出来的，它们在计算机里有特殊用处，比如数字7代表使得计算机发出beep声音。32～127表示的是可打印字符。

**我必须要强调ASCII字符集所要表达的抽象的东西，它只是定义了一种从`数字`到`字符`的映射关系。**我们知道，在这个映射关系里面，有两种对象，一种是数字，一种是字符。但目前，无论是数字，还是字符，都是“虚”的东西，他们都只是映射关系约定之中的一员。如果要实现ASCII字符集所描述的东西，就要将“虚”的变成“实”的：

- 映射关系中的字符（虚） ---> 在屏幕画出字符/打印机打印出字符（实）；
- 映射关系中的数字（虚） ---> 在内存中表示数字（实）。

如何在屏幕画出A这个字符的图案是GUI toolkit的任务，如何在纸上打印出A这个字符图案是打印机的任务。但无论是GUI toolkit还是打印机，要想完成任务，就先要获取到数字65（即数字65需要被加载到内存），然后才能根据映射关系（即字符集）画出或打印出字符A。如何在内存中表示65等这些数字就是我们接下来要讨论的主题：字符编码（下文直接简称编码）。

---
到了这里，有些东西应该是清晰的：

> 我们目前讨论的ASCII码只需要7个bit就可以完全表示（懂二进制的人都知道7个bit就能表示0～127之间的数字）。

现在要考虑的问题是：如果要在内存中表示某个ASCII码表中的数字，需要多少个bit？答案是：大于或等于7个bit即可。

所以，假如我们要在内存表示65，有很多"编排"方式（如果内存足够大，其实是无数种）：

1.  100 0001：占用7个bit；
2. 0100 0001：占用8个bit，1个字节；
3. 0000 0000 0100 0001：占用16个bit，2个字节；
4. 0000 0000 0000 0000 0000 0000 0100 0001：占用32个bit，4个字节；
......



`100 0001`,`0100 0001`等这些二进制组合就是字节序列。将映射关系中的数字变成字节序列的过程就是编码。

在历史选择和那时的硬件发展水平之下，一个字节=8个位（8-bits=1byte），因此，上面说的第二中编码方式（65 ----> 0100 0001）名正言顺成为ASCII码的正式编码方式，这种编码方式的特点就是每次编码8个字节（也就是所谓的定长编码），我们一般直接将这种方式称之为`ASCII编码`;

---

我们可以列出一些已知条件：

- ASCII字符集只需要7个bit；
- ASCII编码是以8-bits为单元去编码的；
- 二进制的7个位表示的范围是：000 0000～111 1111；换成十进制是：0～127
- 二进制的8个位表示的范围是：0000 0000～1111 1111；换成十进制是：0～255；

我觉得有些东西已经很明显了：**使用ASCII编码去编码ASCII字符集的时候，是有一个bit空闲下来的，这个空闲位使得我们拥有128～255的空间**。聪明的人肯定不止一个，那时候就有一批人盯上了这个空闲位，他们各自对从128到255的空间应该如何使用有自己的想法：

> IBM-PC有一种被称为OEM字符集的东西，它提供了一些针对欧洲语言的重音符号和一堆线描字符（各种款识的水平条，垂直条），然后您可以使用这些线条绘制字符在屏幕上制作漂亮的框和线条。实际在计算机商业化的初期，人们在美国以外的地方购买PC时，就构想了各种用途的类OEM字符集，它们都将128～255这个空间段用于自己的目的。例如，在某些PC上，字符代码130将显示为é，但是在以色列出售的计算机上，该字符是希伯来字母Gimel（ג），在很多情况下，不同厂商对大于128对应的字符的处理方法有很多不同的做法。

为了保证计算机始终能够显示英语，ANSI标准出来了：在ANSI标准里，规定0～127表示的就是ASCII字符集里面的内容，但是对于**128以及128以上**的数字所映射的字符，取决于你的国家（这里有些东西已经悄然变化，从`128～255`变成了`128以及128以上`,最明显的原因就是有些国家的文字需要用两个字节才能表示）。

因此就形成了大量的编码系统（编码系统被称为code pages）。这些编码系统有一个共同的特点：**它们都能正确显示英文和本地文字**。

只能够处理英语和本地文字肯定是不够的，因此随后各个标准和升级版编码系统不断出台，为的无非就是一件事：让自家的电脑能够尽量显示更多文字。

各个国家都有各自的编码系统，越来越乱。因此，unicode顺势而来！

### ASCII总结

在讨论unicode之前，我们要先总结一些内容：

1. ASCII字符集是抽象的，它只是定义了一种从数字到字符的映射；
2. 把ASCII字符集打印出来，就是ASCII码表；
3. 将ASCII字符集里的数字在内存中表示出来的过程就是编码；
4. 在内存里读取到字节序列后如何在屏幕显示出来是GUI toolkit的任务；
5. 编码的方式有很多，而以8bits为单位的`定长编码`的叫做ASCII编码；

上面洋洋洒洒说了这么多，无非就是解决一个问题：

> 如何让计算机输出我们能够直接阅读的字符（符号）。

---
接下来要解决的问题是如何让计算机**又多又省**地输出字符。

### unicode

什么是unicode？不藏着掖着了，直接上[官方定义](https://www.unicode.org/faq/basic_q.html)：

> Unicode is the universal character encoding, maintained by the Unicode Consortium. This encoding standard provides the basis for processing, storage and interchange of text data in any language in all modern software and information technology protocols.

Unicode为每个字符提供了一个独一无二的数字，平台无关，程序无关，语言无关。

其实我觉的这个官方定义把Unicode是什么已经说的很明白了：Unicode就是一个更大的映射关系（独一无二的数字<--->字符）嘛。它本身是一种抽象的东西，自然平台无关，程序无关，语言无关。

Unicode致力于给世界上所有的字符都附上一个独一无二的数字。这显然是无法完成的任务，所以unicode是不断更新的。

